{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5b1e5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 69ms/step\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Code borrowed from: \n",
    "\"https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer\"\n",
    "\"https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer/blob/master/saved_models/Emotion_Voice_Detection_Model.h5\"\n",
    "\"https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer/blob/master/AudioRecorder.ipynb\"\n",
    "\"\"\"\n",
    "\n",
    "import pyaudio\n",
    "import wave\n",
    "import warnings\n",
    "\n",
    "import ctypes\n",
    "MB_DEFAULT = 0\n",
    "MB_OKCANCEL = 0x00000001\n",
    "MessageBox = ctypes.windll.user32.MessageBoxW\n",
    "#puts out message box, once user hits ok the program begins to take in audio\n",
    "MessageBox(None, 'Start recording.....              ', 'Emotion Detection', MB_OKCANCEL)  \n",
    "   \n",
    "#print(\"live recording\")\n",
    "\n",
    "continue_inference = True\n",
    "while continue_inference:    \n",
    "    #setting formatting for recording 4 second stream\n",
    "    CHUNK = 1024 \n",
    "    FORMAT = pyaudio.paInt16 \n",
    "    CHANNELS = 2 \n",
    "    RATE = 44100 \n",
    "    RECORD_SECONDS = 4\n",
    "    WAVE_OUTPUT_FILENAME = \"output10.wav\"\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    #begins to stream\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK) #buffer\n",
    "\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    #stream over\n",
    "\n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "\n",
    "    import librosa\n",
    "    from librosa.display import waveplot\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    #loads recording into the output10 file\n",
    "    data, sampling_rate = librosa.load('output10.wav')\n",
    "\n",
    "\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import librosa\n",
    "    import glob \n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    #loads the recording file, saves sample rate, and extracts elevant audio features (MFCCs) into numpy array\n",
    "    X, sample_rate = librosa.load('output10.wav', res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "    sample_rate = np.array(sample_rate)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\n",
    "    featurelive = mfccs\n",
    "    livedf2 = featurelive\n",
    "\n",
    "    #data from numpy array --> pandas dataframe \n",
    "    livedf2= pd.DataFrame(data=livedf2)\n",
    "\n",
    "    #alters dataframe formatting so rows become columns and colums become rows\n",
    "    livedf2 = livedf2.stack().to_frame().T\n",
    "\n",
    "    #adds in one more dimension to data\n",
    "    twodim= np.expand_dims(livedf2, axis=2)\n",
    "\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    #loads weights from pretrained model\n",
    "    loaded_model = tf.keras.models.load_model('C:/datasets/day 22/Emotion_Voice_Detection_Model (2).h5')\n",
    "\n",
    "    #model makes predictions on MFCCs\n",
    "    livepreds = loaded_model.predict(twodim, \n",
    "                             batch_size=32, \n",
    "                             verbose=1)\n",
    "    \n",
    "    #saves label predicted by model\n",
    "    livepreds1=livepreds.argmax(axis=1)\n",
    "    \n",
    "    #ensures label formatted as integer\n",
    "    liveabc = livepreds1.astype(int).flatten()\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    lb = LabelEncoder()\n",
    "    #defines encoding emotion labels and has each label correspond to its position on the lists -- e.g. female_angry becomes 0 in int_labels array\n",
    "    str_labels = np.array([\"female_angry\", \"female_calm\", \"female_fearful\", \"female_happy\", \"female_sad\", \"male_angry\", \"male_calm\", \"male_fearful\", \"male_happy\", \"male_sad\"])\n",
    "    int_labels = lb.fit_transform(str_labels)\n",
    "\n",
    "    #integer_label --> string label\n",
    "    livepredictions = (lb.inverse_transform((liveabc)))\n",
    "\n",
    "    #takes string label inside of the array item livepredictions\n",
    "    label = livepredictions[0]\n",
    "        \n",
    "    #print(label.split(\"_\",1)[1]) -- simply outputs emotion without reference to identified gender\n",
    "    \n",
    "    #if user hits cancel the model stops running and taking in audio\n",
    "    should_cancel = MessageBox(None, label.split(\"_\",1)[1], 'Emotion Detection', MB_OKCANCEL) \n",
    "    if should_cancel == 2:\n",
    "        continue_inference = False\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce05890e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
